[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Mosaab Boustta",
    "section": "",
    "text": "üëã Hello there! I‚Äôm Mosaab, a passionate and self-driven computer science student navigating the challenges at 1337 Coding School.\nIn this space, my goal is to offer practical tutorials and guides centered around AI/ML. I firmly believe in the power of hands-on experience, starting with implementation and then delving into comprehensive explanations. My commitment extends to keeping abreast of the latest research, and I take pleasure in sharing my knowledge and insights through the content you‚Äôll find on this blog.\nJoin me on this journey of exploration and learning‚Äîwe‚Äôre bound to have some fun and pick up valuable insights along the way!\nFeel free to reach out via email at this address."
  },
  {
    "objectID": "about.html#professional-journey",
    "href": "about.html#professional-journey",
    "title": "Mosaab Boustta",
    "section": "Professional Journey",
    "text": "Professional Journey\n\nMember Of Technical Staff [Full-time], Oracle, Nov 2022 ‚Äì Dec 2023\nResearch Assistant [Internship], Oracle, May 2022 ‚Äì Nov 2022"
  },
  {
    "objectID": "about.html#academic-pursuits",
    "href": "about.html#academic-pursuits",
    "title": "Mosaab Boustta",
    "section": "Academic Pursuits",
    "text": "Academic Pursuits\n\nIT Architecture Expert (RNCP 36137), Computer Science, 1337 Coding School"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "Mosaab Boustta",
    "section": "Let‚Äôs Connect:",
    "text": "Let‚Äôs Connect:\n\nLinkedIn\n\nTwitter\n\nGithub"
  },
  {
    "objectID": "posts/llm-finetuning-guide/LLM Finetuning Guide.html",
    "href": "posts/llm-finetuning-guide/LLM Finetuning Guide.html",
    "title": "A Beginner‚Äôs Guide to LLM Fine-Tuning",
    "section": "",
    "text": "The growing interest in Large Language Models (LLMs) has led to a surge in tools and wrappers designed to streamline their training process.\nPopular options include FastChat from LMSYS (used to train Vicuna) and Hugging Face‚Äôs transformers/trl libraries (used in my previous article). In addition, each big LLM project, like WizardLM, tends to have its own training script, inspired by the original Alpaca implementation.\nIn this article, we will use Axolotl, a tool created by the OpenAccess AI Collective. We will use it to fine-tune a Code Llama 7b model on an evol-instruct dataset comprised of 1,000 samples of Python code."
  },
  {
    "objectID": "posts/llm-finetuning-guide/LLM Finetuning Guide.html#why-axolotl",
    "href": "posts/llm-finetuning-guide/LLM Finetuning Guide.html#why-axolotl",
    "title": "A Beginner‚Äôs Guide to LLM Fine-Tuning",
    "section": "ü§î Why Axolotl?",
    "text": "ü§î Why Axolotl?\nThe main appeal of Axolotl is that it provides a one-stop solution, which includes numerous features, model architectures, and an active community. Here‚Äôs a quick list of my favorite things about it:\n\nConfiguration: All parameters used to train an LLM are neatly stored in a yaml config file. This makes it convenient for sharing and reproducing models. You can see an example for Llama 2 here.\nDataset Flexibility: Axolotl allows the specification of multiple datasets with varied prompt formats such as alpaca ({\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}), sharegpt:chat ({\"conversations\": [{\"from\": \"...\", \"value\": \"...\"}]}), and raw completion ({\"text\": \"...\"}). Combining datasets is seamless, and the hassle of unifying the prompt format is eliminated.\nFeatures: Axolotl is packed with SOTA techniques such as FSDP, deepspeed, LoRA, QLoRA, ReLoRA, sample packing, GPTQ, FlashAttention, xformers, and rope scaling.\nUtilities: There are numerous user-friendly utilities integrated, including the addition or alteration of special tokens, or a custom wandb configuration.\n\nSome well-known models trained using this tool are Manticore-13b from the OpenAccess AI Collective and Samantha-1.11-70b from Eric Hartford. Like other wrappers, it is built on top of the transformers library and uses many of its features."
  },
  {
    "objectID": "posts/llm-finetuning-guide/LLM Finetuning Guide.html#create-your-own-config-file",
    "href": "posts/llm-finetuning-guide/LLM Finetuning Guide.html#create-your-own-config-file",
    "title": "A Beginner‚Äôs Guide to LLM Fine-Tuning",
    "section": "‚öôÔ∏è Create your own config file",
    "text": "‚öôÔ∏è Create your own config file\nBefore anything, we need a configuration file. You can reuse an existing configuration from the examples folder. In our case, we will tweak the QLoRA config for Llama 2 to create our own Code Llama model. The model will be trained on a subset of 1,000 Python samples from the nickrosh/Evol-Instruct-Code-80k-v1 dataset.\nFirst, we must change the base_model and base_model_config fields to ‚Äúcodellama/CodeLlama-7b-hf‚Äù. To push our trained adapter to the Hugging Face Hub, let‚Äôs add a new field hub_model_id, which corresponds to the name of our model, ‚ÄúEvolCodeLlama-7b‚Äù. Now, we have to update the dataset to mlabonne/Evol-Instruct-Python-1k and set type to ‚Äúalpaca‚Äù.\nThere‚Äôs no sample bigger than 2048 tokens in this dataset, so we can reduce the sequence_len to ‚Äú2048‚Äù and save some VRAM. Talking about VRAM, we‚Äôre going to use a micro_batch_size of 10 and a gradient_accumulation_steps of 1 to maximize its use. In practice, you try different values until you use &gt;95% of the available VRAM.\nFor convenience, I‚Äôm going to add the name ‚Äúaxolotl‚Äù to the wandb_project field so it‚Äôs easier to track on my account. I‚Äôm also setting the warmup_steps to ‚Äú100‚Äù (personal preference) and the eval_steps to 0.01 so we‚Äôll end up with 100 evaluations.\nHere‚Äôs how the final config file should look:\nbase_model: codellama/CodeLlama-7b-hf\nbase_model_config: codellama/CodeLlama-7b-hf\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\nis_llama_derived_model: true\nhub_model_id: EvolCodeLlama-7b\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n    - path: mlabonne/Evol-Instruct-Python-1k\n    type: alpaca\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.02\noutput_dir: ./qlora-out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 2048\nsample_packing: true\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project: axolotl\nwandb_entity:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\ngradient_accumulation_steps: 1\nmicro_batch_size: 10\nnum_epochs: 3\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: true\nfp16: false\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 100\neval_steps: 0.01\nsave_strategy: epoch\nsave_steps:\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n    bos_token: \"&lt;s&gt;\"\n    eos_token: \"&lt;/s&gt;\"\n    unk_token: \"&lt;unk&gt;\"\nYou can also find this config file here as a GitHub gist.\nBefore we start training our model, I want to introduce a few parameters that are important to understand:\n\nQLoRA: We‚Äôre using QLoRA for fine-tuning, which is why we‚Äôre loading the base model in 4-bit precision (NF4 format). You can check this article from Benjamin Marie to know more about QLoRA.\nGradient checkpointing: It lowers the VRAM requirements by removing some activations that are re-computed on demand during the backward pass. It also slows down training by about 20%, according to Hugging Face‚Äôs documentation.\nFlashAttention: This implements the FlashAttentionmechanism, which improves the speed and memory efficiency of our model thanks to a clever fusion of GPU operations (learn more about it in this article from Aleksa Gordi√ß).\nSample packing: Smart way of creating batches with as little padding as possible, by reorganizing the order of the samples (bin packing problem). As a result, we need fewer batches to train the model on the same dataset. It was inspired by the Multipack Sampler (see my note) and Krell et al.\n\nYou can find FlashAttention in some other tools, but sample packing is relatively new. As far as I know, OpenChatwas the first project to use sample packing during fine-tuning. Thanks to Axolotl, we‚Äôll use these techniques for free."
  },
  {
    "objectID": "posts/llm-finetuning-guide/LLM Finetuning Guide.html#fine-tune-code-llama",
    "href": "posts/llm-finetuning-guide/LLM Finetuning Guide.html#fine-tune-code-llama",
    "title": "A Beginner‚Äôs Guide to LLM Fine-Tuning",
    "section": "ü¶ô Fine-tune Code Llama",
    "text": "ü¶ô Fine-tune Code Llama\nHaving the config file ready, it‚Äôs time to get our hands dirty with the actual fine-tuning. You might consider running the training on a Colab notebook. However, for those without access to a high-performance GPU, a more cost-effective solution consists of renting cloud-based GPU services, like AWS, Lambda Labs, Vast.ai, Banana, or RunPod.\nPersonally, I use RunPod, which is a popular option in the fine-tuning community. It‚Äôs not the cheapest service but it hits a good tradeoff with a clean UI. You can easily replicate the following steps using your favorite service.\nWhen your RunPod account is set up, go to Manage &gt; Templates and click on ‚ÄúNew Template‚Äù. Here is a simple template:\n\n\n\nLet‚Äôs review the different fields and their corresponding values:\n\nTemplate Name: Axolotl (you can choose whatever you want)\nContainer Image: winglian/axolotl-runpod:main-py3.10-cu118-2.0.1\nContainer Disk: 100 GB\nVolume Disk: 0 GB\nVolume Mount Path: /workspace\n\nIn addition, there are two handy environment variables can include:\n\nHUGGING_FACE_HUB_TOKEN: you can find your token on this page (requires an account)\nWANDB_API_KEY: you can find your key on this page (requires an account)\n\nAlternatively, you can simply log in the terminal later (using huggingface-cli login and wandb login). Once you‚Äôre set-up, go to Community Cloud and deploy an RTX 3090. Here you can search for the name of your template and select it as follows:\n\n\n\nYou can click on ‚ÄúContinue‚Äù and RunPod will deploy your template. You can see the installation in your pod‚Äôs logs (Manage &gt; Pods). When the option becomes available, click on ‚ÄúConnect‚Äù. Here, click on ‚Äútart Web Terminal‚Äù and then ‚ÄúConnect to Web Terminal‚Äù. You are now connected to your pod!\nThe following steps are the same no matter what service you choose:\n\nWe install Axolotl and the PEFT library as follows:\n\ngit clone https://github.com/OpenAccess-AI-Collective/axolotl\ncd axolotl\n\npip3 install -e .[flash-attn]\npip3 install -U git+https://github.com/huggingface/peft.git\n\nDownload the config file we created:\n\nwget https://gist.githubusercontent.com/mlabonne/8055f6335e2b85f082c8c75561321a66/raw/93915a9563fcfff8df9a81fc0cdbf63894465922/EvolCodeLlama-7b.yaml\n\nYou can now start fine-tuning the model with the following command:\n\naccelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml\nIf everything is configured correctly, you should be able to train the model in a little more than one hour (it took me 1h 11m 44s). If you check the GPU memory used, you‚Äôll see almost 100% with this config, which means we‚Äôre optimizing it pretty nicely. If you‚Äôre using a GPU with more VRAM (like an A100), you can increase the micro-batch size to make sure you‚Äôre fully using it.\nIn the meantime, feel free to close the web terminal and check your loss on Weights & Biases. We‚Äôre using tmux so the training won‚Äôt stop if you close the terminal. Here are my loss curves:\n\n\n\nWe see a steady improvement in the eval loss, which is a good sign. However, you can also spot drops in the eval loss that are not correlated with a decrease in the quality of the outputs. The best way to evaluate your model is simply by using it: you can run it in the terminal with the command accelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml ‚Äìinference ‚Äìlora_model_dir=‚Äú./qlora-out‚Äù.\nThe QLoRA adapter should already be uploaded to the Hugging Face Hub. However, you can also merge the base Code Llama model with this adapter and push the merged model there by following these steps:\n\nDownload this script:\n\nwget https://gist.githubusercontent.com/mlabonne/a3542b0519708b8871d0703c938bba9f/raw/60abc5afc07f9d843bc23d56f4e0b7ab072c4a62/merge_peft.py\n\nExecute it with this command:\n\npython merge_peft.py --base_model=codellama/CodeLlama-7b-hf --peft_model=./qlora-out --hub_id=EvolCodeLlama-7b\nCongratulations, you should have your own EvolCodeLlama-7b on the Hugging Face Hub at this point! For reference, you can access my own model trained with this process here: mlabonne/EvolCodeLlama-7b\nConsidering that our EvolCodeLlama-7b is a code LLM, it would be interesting to compare its performance with other models on standard benchmarks, such as HumanEval and MBPP. For reference, you can find a leaderboard at the following address: Multilingual Code Evals.\nIf you‚Äôre happy with this model, you can quantize it with GGML for local inference with this free Google Colab notebook. You can also fine-tune bigger models (e.g., 70b parameters) thanks to deepspeed, which only requires an additional config file."
  },
  {
    "objectID": "posts/llm-finetuning-guide/LLM Finetuning Guide.html#conclusion",
    "href": "posts/llm-finetuning-guide/LLM Finetuning Guide.html#conclusion",
    "title": "A Beginner‚Äôs Guide to LLM Fine-Tuning",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we‚Äôve covered the essentials of how to efficiently fine-tune LLMs. We customized parameters to train on our Code Llama model on a small Python dataset. Finally, we merged the weights and uploaded the result on Hugging Face.\nI hope you found this guide useful. I recommend using Axolotl with a cloud-based GPU service to get some experience and upload a few models on Hugging Face. Build your own datasets, play with the parameters, and break stuff along the way. Like with every wrapper, don‚Äôt hesitate to check the source code to get a good intuition of what it‚Äôs actually doing. It will massively help in the long run.\nThanks to the OpenAccess AI Collective and all the contributors!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Blog",
    "section": "",
    "text": "A Beginner‚Äôs Guide to LLM Fine-Tuning\n\n\nHow to fine-tune Llama and other LLMs with one tool\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2023\n\n\n\n\n\n\nNo matching items"
  }
]